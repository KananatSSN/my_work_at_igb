{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable before importing numpy/sklearn\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d519fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = r\"D:\\IFCB_05_20\"\n",
    "output_folder_path = r\"D:\\test0\"\n",
    "\n",
    "number_of_clusters = 5\n",
    "number_of_elements_in_clusters = 6\n",
    "\n",
    "# length of depth_map should be equal to number_of_elements_in_clusters\n",
    "folder_name_map = {\n",
    "    0 : '2_m',\n",
    "    1 : '6.5_m',\n",
    "    2 : '7_m',\n",
    "    3 : '8_m',\n",
    "    4 : '10_m',\n",
    "    5 : '16_m' # assuming 6 elements in total\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed35d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timing = list({file.stem.split(\"_\")[0] for file in Path(input_folder_path).iterdir() if file.is_file()})\n",
    "\n",
    "# Convert to Unix timestamps and sort\n",
    "timing_unix = [convert_to_unix_time(time) for time in timing]\n",
    "timing_unix = sorted([(time-min(timing_unix)) for time in timing_unix])\n",
    "\n",
    "print(f\"timing_unix: {timing_unix}\")\n",
    "plt.plot(timing_unix, 'o-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d83271",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(i, timing_unix[i]) for i in range(len(timing_unix))]\n",
    "slope = (timing_unix[1] - timing_unix[0])\n",
    "\n",
    "distance_list = []\n",
    "for point in points:\n",
    "    dist = distance_point_to_line_slope_intercept(point, slope)\n",
    "    distance_list.append(dist)\n",
    "\n",
    "distance_list = sorted(distance_list)\n",
    "print(f\"distance_list: {distance_list}\")\n",
    "\n",
    "plt.plot(distance_list, 'o-')\n",
    "\n",
    "#[(0, [2]), (1, [4]), (2, [3]), (4, [2, 3, 4, 5])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "\n",
    "def kmeans_1d_clustering(data, k=3, plot=True, find_optimal_k=False, max_k=10):\n",
    "    \"\"\"\n",
    "    Apply K-means clustering to 1D data and visualize results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : list or array-like\n",
    "        1D list or array of numbers\n",
    "    k : int, default=3\n",
    "        Number of clusters\n",
    "    plot : bool, default=True\n",
    "        Whether to create visualizations\n",
    "    find_optimal_k : bool, default=False\n",
    "        Whether to find optimal k using elbow method and silhouette score\n",
    "    max_k : int, default=10\n",
    "        Maximum k to test when finding optimal k\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing clustering results\n",
    "        - 'labels': cluster labels for each data point\n",
    "        - 'centers': cluster centers\n",
    "        - 'inertia': within-cluster sum of squares\n",
    "        - 'silhouette_score': silhouette score (if k > 1)\n",
    "        - 'optimal_k': optimal k (if find_optimal_k=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy array and reshape for sklearn\n",
    "    data = np.array(data)\n",
    "    X = data.reshape(-1, 1)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Find optimal k if requested\n",
    "    if find_optimal_k:\n",
    "        optimal_k = find_optimal_clusters(X, max_k, plot)\n",
    "        results['optimal_k'] = optimal_k\n",
    "        k = optimal_k\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_.flatten()\n",
    "    \n",
    "    # Store results\n",
    "    results.update({\n",
    "        'labels': labels,\n",
    "        'centers': centers,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'silhouette_score': silhouette_score(X, labels) if k > 1 else None\n",
    "    })\n",
    "    \n",
    "    # Create visualizations\n",
    "    if plot:\n",
    "        create_visualizations(data, labels, centers, k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def find_optimal_clusters(X, max_k, plot=True):\n",
    "    \"\"\"Find optimal number of clusters using elbow method and silhouette analysis.\"\"\"\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "    \n",
    "    if plot:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Elbow method plot\n",
    "        ax1.plot(k_range, inertias, 'bo-')\n",
    "        ax1.set_xlabel('Number of Clusters (k)')\n",
    "        ax1.set_ylabel('Inertia (Within-cluster Sum of Squares)')\n",
    "        ax1.set_title('Elbow Method for Optimal k')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Silhouette score plot\n",
    "        ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "        ax2.set_xlabel('Number of Clusters (k)')\n",
    "        ax2.set_ylabel('Silhouette Score')\n",
    "        ax2.set_title('Silhouette Analysis for Optimal k')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Find optimal k (highest silhouette score)\n",
    "    optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "    return optimal_k\n",
    "\n",
    "def create_visualizations(data, labels, centers, k):\n",
    "    \"\"\"Create multiple visualizations of the clustering results.\"\"\"\n",
    "    \n",
    "    # Set up colors\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, k))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Scatter plot with clusters\n",
    "    ax1 = axes[0, 0]\n",
    "    for i in range(k):\n",
    "        cluster_data = data[labels == i]\n",
    "        ax1.scatter(cluster_data, [i] * len(cluster_data), \n",
    "                   c=[colors[i]], label=f'Cluster {i}', alpha=0.7, s=50)\n",
    "    \n",
    "    # Plot centers\n",
    "    for i, center in enumerate(centers):\n",
    "        ax1.axvline(x=center, color=colors[i], linestyle='--', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Data Values')\n",
    "    ax1.set_ylabel('Cluster')\n",
    "    ax1.set_title('K-means Clustering Results')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Histogram with cluster colors\n",
    "    ax2 = axes[0, 1]\n",
    "    for i in range(k):\n",
    "        cluster_data = data[labels == i]\n",
    "        ax2.hist(cluster_data, bins=20, alpha=0.6, color=colors[i], \n",
    "                label=f'Cluster {i}', edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    for i, center in enumerate(centers):\n",
    "        ax2.axvline(x=center, color=colors[i], linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Data Values')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Clusters')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Line plot showing data points in order\n",
    "    ax3 = axes[1, 0]\n",
    "    for i in range(k):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        ax3.scatter(cluster_indices, data[cluster_indices], \n",
    "                   c=[colors[i]], label=f'Cluster {i}', alpha=0.7, s=30)\n",
    "    \n",
    "    ax3.plot(range(len(data)), data, 'k-', alpha=0.3, linewidth=1)\n",
    "    ax3.set_xlabel('Data Point Index')\n",
    "    ax3.set_ylabel('Data Value')\n",
    "    ax3.set_title('Clusters by Original Order')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Box plot of clusters\n",
    "    ax4 = axes[1, 1]\n",
    "    cluster_data = [data[labels == i] for i in range(k)]\n",
    "    bp = ax4.boxplot(cluster_data, patch_artist=True, labels=[f'Cluster {i}' for i in range(k)])\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax4.set_xlabel('Cluster')\n",
    "    ax4.set_ylabel('Data Values')\n",
    "    ax4.set_title('Box Plot of Clusters')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print cluster statistics\n",
    "    print(\"\\nCluster Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(k):\n",
    "        cluster_data = data[labels == i]\n",
    "        print(f\"Cluster {i}:\")\n",
    "        print(f\"  Size: {len(cluster_data)}\")\n",
    "        print(f\"  Center: {centers[i]:.3f}\")\n",
    "        print(f\"  Mean: {np.mean(cluster_data):.3f}\")\n",
    "        print(f\"  Std: {np.std(cluster_data):.3f}\")\n",
    "        print(f\"  Range: [{np.min(cluster_data):.3f}, {np.max(cluster_data):.3f}]\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    sample_data = distance_list\n",
    "    \n",
    "    print(\"Example 1: Basic clustering with k=3\")\n",
    "    results = kmeans_1d_clustering(sample_data, k=5)\n",
    "    \n",
    "    print(f\"Inertia: {results['inertia']:.3f}\")\n",
    "    print(f\"Silhouette Score: {results['silhouette_score']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ec5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_time(date_string):\n",
    "    \"\"\"\n",
    "    Convert date string in format 'D20250520T000334' to Unix timestamp.\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): Date in format DYYYYMMDDTHHMMSS\n",
    "    \n",
    "    Returns:\n",
    "        int: Unix timestamp\n",
    "    \"\"\"\n",
    "    # Remove the 'D' prefix and split date and time parts\n",
    "    clean_string = date_string[1:]  # Remove 'D'\n",
    "    date_part = clean_string[:8]    # YYYYMMDD\n",
    "    time_part = clean_string[9:]    # HHMMSS (skip the 'T')\n",
    "    \n",
    "    # Parse the datetime\n",
    "    dt = datetime.strptime(date_part + time_part, '%Y%m%d%H%M%S')\n",
    "    \n",
    "    # Convert to Unix timestamp\n",
    "    return int(dt.timestamp())\n",
    "\n",
    "def unix_to_original_format(unix_timestamp):\n",
    "    \"\"\"\n",
    "    Convert Unix timestamp back to original format 'DYYYYMMDDTHHMMSS'.\n",
    "    \n",
    "    Args:\n",
    "        unix_timestamp (int or float): Unix timestamp\n",
    "    \n",
    "    Returns:\n",
    "        str: Date string in format DYYYYMMDDTHHMMSS\n",
    "    \"\"\"\n",
    "    # Convert Unix timestamp to datetime object\n",
    "    dt = datetime.fromtimestamp(unix_timestamp)\n",
    "    \n",
    "    # Format as DYYYYMMDDTHHMMSS\n",
    "    formatted_date = dt.strftime('D%Y%m%dT%H%M%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def distance_point_to_line_slope_intercept(point, slope):\n",
    "    \"\"\"\n",
    "    Calculate distance from point to line y = mx + b.\n",
    "    \n",
    "    Args:\n",
    "        point: (x, y) coordinates of the point\n",
    "        slope: slope of the line (m)\n",
    "        intercept: y-intercept of the line (b)\n",
    "    \n",
    "    Returns:\n",
    "        Distance from point to line\n",
    "    \"\"\"\n",
    "    x0, y0 = point\n",
    "    \n",
    "    # Convert y = mx + b to ax + by + c = 0 form: mx - y + b = 0\n",
    "    a = slope\n",
    "    b = -1\n",
    "    \n",
    "    # Distance formula: |ax0 + by0 + c| / sqrt(a^2 + b^2)\n",
    "    distance = abs(a * x0 + b * y0 ) / np.sqrt(a**2 + b**2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def find_small_clusters(data, n_clusters=5, min_size=6):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering, sort clusters by center, and return small clusters info.\n",
    "    \n",
    "    Args:\n",
    "        data: List or array of data points\n",
    "        n_clusters: Number of clusters for K-means\n",
    "        min_size: Minimum required cluster size\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(sorted_cluster_index, actual_size), ...]\n",
    "        Only includes clusters with size < min_size\n",
    "    \"\"\"\n",
    "    # Convert to numpy array and perform K-means\n",
    "    X = np.array(data).reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_.flatten()\n",
    "    \n",
    "    # Count cluster sizes\n",
    "    cluster_counts = Counter(labels)\n",
    "\n",
    "    # print(f\"Cluster counts: {cluster_counts}\")\n",
    "    \n",
    "    # Create mapping of original cluster index to sorted index\n",
    "    # Sort clusters by their center values\n",
    "    cluster_center_pairs = [(i, centers[i]) for i in range(n_clusters)]\n",
    "\n",
    "    # print(f\"Cluster center pairs before sorting: {cluster_center_pairs}\")\n",
    "\n",
    "    sorted_clusters = sorted(cluster_center_pairs, key=lambda x: x[1])\n",
    "\n",
    "    # print(f\"Sorted cluster center pairs: {sorted_clusters}\")\n",
    "    \n",
    "    # Create mapping: original_index -> sorted_index\n",
    "    original_to_sorted = {original_idx: sorted_idx \n",
    "                         for sorted_idx, (original_idx, _) in enumerate(sorted_clusters)}\n",
    "    \n",
    "    # Find clusters with size < min_size and return with sorted indices\n",
    "    small_clusters = []\n",
    "    for original_idx, size in cluster_counts.items():\n",
    "        if size < min_size:\n",
    "            sorted_idx = original_to_sorted[original_idx]\n",
    "            small_clusters.append((sorted_idx, size))\n",
    "    # print(f\"Small clusters before sorting: {small_clusters}\")\n",
    "    # Sort the result by sorted cluster index\n",
    "    small_clusters.sort(key=lambda x: x[0])\n",
    "    # print(f\"Small clusters after sorting: {small_clusters}\")\n",
    "    \n",
    "    return small_clusters\n",
    "\n",
    "def gap_analysis(data, number_of_elements_in_clusters=6, tolerance=1.5):\n",
    "    # print(f\"Analyzing gap in data: {data}\")\n",
    "    gaps = [data[i + 1] - data[i] for i in range(len(data) - 1)]\n",
    "    avg_gap = sum(gaps) / len(gaps)\n",
    "\n",
    "    hole_found = 0\n",
    "\n",
    "    # Identify gaps larger than average gap multiplied by tolerance\n",
    "    for gap in gaps:\n",
    "        if gap > (avg_gap * tolerance):\n",
    "            #print(f\"Holes of size {gap} found (average gap: {avg_gap})\")\n",
    "            data.insert((gaps.index(gap) + 1), 'missing_data')\n",
    "            hole_found += 1\n",
    "\n",
    "    # print(f\"Number of holes found: {hole_found}\")\n",
    "    \n",
    "    # If you don't find the hole in data then it must be at the end\n",
    "    if hole_found != (number_of_elements_in_clusters - len(data)):\n",
    "        for i in range(number_of_elements_in_clusters - len(data) - hole_found):\n",
    "            data.append('missing_data')\n",
    "            \n",
    "    return [i for i, value in enumerate(data) if value == 'missing_data']\n",
    "\n",
    "def find_missing_data_points(input_folder_path, number_of_clusters = 5, number_of_elements_in_clusters = 6):\n",
    "\n",
    "    # Extract date/time\n",
    "    timing = list({file.stem.split(\"_\")[0] for file in Path(input_folder_path).iterdir() if file.is_file()})\n",
    "\n",
    "    # Convert to Unix timestamps and sort\n",
    "    timing_unix = [convert_to_unix_time(time) for time in timing]\n",
    "    timing_unix = sorted([(time-min(timing_unix)) for time in timing_unix])\n",
    "\n",
    "    # Convert to distance to make clustering more consistent\n",
    "    points = [(i, timing_unix[i]) for i in range(len(timing_unix))]\n",
    "    slope = (timing_unix[1] - timing_unix[0])\n",
    "\n",
    "    distance_list = []\n",
    "    for point in points:\n",
    "        dist = distance_point_to_line_slope_intercept(point, slope)\n",
    "        distance_list.append(dist)\n",
    "    \n",
    "    # Perform K-means clustering to find which cluster has less than expected number of elements\n",
    "    clustering_results = find_small_clusters(distance_list, n_clusters=number_of_clusters, min_size=number_of_elements_in_clusters)\n",
    "\n",
    "    print(f\"Clustering results: {clustering_results}\")\n",
    "    \n",
    "    # Analyze gap in each cluster for missing data points\n",
    "    missing_data = []\n",
    "\n",
    "    processed_data = 0\n",
    "    for cluster_index, cluster_size in clustering_results:\n",
    "\n",
    "        # print(f\"Processing Cluster {cluster_index} with size {cluster_size}\")\n",
    "        start_idx = processed_data\n",
    "        # print(f\"start_idx: {start_idx}\")\n",
    "        end_idx = start_idx + cluster_size\n",
    "        # print(f\"end_idx: {end_idx}\")\n",
    "\n",
    "        cluster_data = timing_unix[start_idx:end_idx]\n",
    "        missing_data_points_index = gap_analysis(cluster_data, number_of_elements_in_clusters)\n",
    "        \n",
    "        missing_data.append((cluster_index, missing_data_points_index))\n",
    "        # print(f\"Missing data point(s) at position {missing_data_points_index} in Cluster {cluster_index} \")\n",
    "        processed_data += cluster_size\n",
    "    \n",
    "    return missing_data\n",
    "\n",
    "print(find_missing_data_points(input_folder_path, number_of_clusters, number_of_elements_in_clusters))\n",
    "\n",
    "# [0, 2352, 6453, 8606, 11282, 17982, 22344, 24413, 26561, 29215, 35985, 38321, 40344, 42413, 44562, 53977, 56318, 58342, 60412, 62563, 65225, 71979, 74325, 76348, 78426, 80575]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ac08079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results: [(0, 5), (1, 5), (2, 5), (4, 5)]\n",
      "Missing data indices: [5, 7, 17, 29]\n",
      "Sorted timing with missing data: [0, 2352, 6453, 8606, 11282, 'missing_data', 17982, 22344, 'missing_data', 24413, 26561, 29215, 35985, 38321, 40344, 42413, 44562, 53977, 56318, 'missing_data', 58342, 60412, 62563, 65225, 71979, 74325, 76348, 78426, 80575, 'missing_data']\n",
      "length: 30\n",
      "0: D20250520T134448\n",
      "Copy D:\\IFCB_05_20\\D20250520T134448_IFCB135.roi to D:\\test0\\20250520\\2_m\\D20250520T134448_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T134448_IFCB135.hdr to D:\\test0\\20250520\\2_m\\D20250520T134448_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T134448_IFCB135.adc to D:\\test0\\20250520\\2_m\\D20250520T134448_IFCB135.adc\n",
      "1: D20250520T142400\n",
      "Copy D:\\IFCB_05_20\\D20250520T142400_IFCB135.hdr to D:\\test0\\20250520\\6.5_m\\D20250520T142400_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T142400_IFCB135.roi to D:\\test0\\20250520\\6.5_m\\D20250520T142400_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T142400_IFCB135.adc to D:\\test0\\20250520\\6.5_m\\D20250520T142400_IFCB135.adc\n",
      "2: D20250520T153221\n",
      "Copy D:\\IFCB_05_20\\D20250520T153221_IFCB135.adc to D:\\test0\\20250520\\7_m\\D20250520T153221_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250520T153221_IFCB135.roi to D:\\test0\\20250520\\7_m\\D20250520T153221_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T153221_IFCB135.hdr to D:\\test0\\20250520\\7_m\\D20250520T153221_IFCB135.hdr\n",
      "3: D20250520T160814\n",
      "Copy D:\\IFCB_05_20\\D20250520T160814_IFCB135.adc to D:\\test0\\20250520\\8_m\\D20250520T160814_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250520T160814_IFCB135.hdr to D:\\test0\\20250520\\8_m\\D20250520T160814_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T160814_IFCB135.roi to D:\\test0\\20250520\\8_m\\D20250520T160814_IFCB135.roi\n",
      "4: D20250520T165250\n",
      "Copy D:\\IFCB_05_20\\D20250520T165250_IFCB135.adc to D:\\test0\\20250520\\10_m\\D20250520T165250_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250520T165250_IFCB135.roi to D:\\test0\\20250520\\10_m\\D20250520T165250_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T165250_IFCB135.hdr to D:\\test0\\20250520\\10_m\\D20250520T165250_IFCB135.hdr\n",
      "5: missing_data\n",
      "6: D20250520T184430\n",
      "Copy D:\\IFCB_05_20\\D20250520T184430_IFCB135.roi to D:\\test0\\20250520\\2_m\\D20250520T184430_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T184430_IFCB135.hdr to D:\\test0\\20250520\\2_m\\D20250520T184430_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T184430_IFCB135.adc to D:\\test0\\20250520\\2_m\\D20250520T184430_IFCB135.adc\n",
      "7: D20250520T195712\n",
      "Copy D:\\IFCB_05_20\\D20250520T195712_IFCB135.hdr to D:\\test0\\20250520\\6.5_m\\D20250520T195712_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T195712_IFCB135.roi to D:\\test0\\20250520\\6.5_m\\D20250520T195712_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T195712_IFCB135.adc to D:\\test0\\20250520\\6.5_m\\D20250520T195712_IFCB135.adc\n",
      "8: missing_data\n",
      "9: D20250520T203141\n",
      "Copy D:\\IFCB_05_20\\D20250520T203141_IFCB135.adc to D:\\test0\\20250520\\8_m\\D20250520T203141_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250520T203141_IFCB135.roi to D:\\test0\\20250520\\8_m\\D20250520T203141_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T203141_IFCB135.hdr to D:\\test0\\20250520\\8_m\\D20250520T203141_IFCB135.hdr\n",
      "10: D20250520T210729\n",
      "Copy D:\\IFCB_05_20\\D20250520T210729_IFCB135.adc to D:\\test0\\20250520\\10_m\\D20250520T210729_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250520T210729_IFCB135.hdr to D:\\test0\\20250520\\10_m\\D20250520T210729_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T210729_IFCB135.roi to D:\\test0\\20250520\\10_m\\D20250520T210729_IFCB135.roi\n",
      "11: D20250520T215143\n",
      "Copy D:\\IFCB_05_20\\D20250520T215143_IFCB135.roi to D:\\test0\\20250520\\16_m\\D20250520T215143_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T215143_IFCB135.hdr to D:\\test0\\20250520\\16_m\\D20250520T215143_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T215143_IFCB135.adc to D:\\test0\\20250520\\16_m\\D20250520T215143_IFCB135.adc\n",
      "12: D20250520T234433\n",
      "Copy D:\\IFCB_05_20\\D20250520T234433_IFCB135.roi to D:\\test0\\20250520\\2_m\\D20250520T234433_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250520T234433_IFCB135.hdr to D:\\test0\\20250520\\2_m\\D20250520T234433_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250520T234433_IFCB135.adc to D:\\test0\\20250520\\2_m\\D20250520T234433_IFCB135.adc\n",
      "13: D20250521T002329\n",
      "Copy D:\\IFCB_05_20\\D20250521T002329_IFCB135.adc to D:\\test0\\20250521\\6.5_m\\D20250521T002329_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T002329_IFCB135.roi to D:\\test0\\20250521\\6.5_m\\D20250521T002329_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T002329_IFCB135.hdr to D:\\test0\\20250521\\6.5_m\\D20250521T002329_IFCB135.hdr\n",
      "14: D20250521T005712\n",
      "Copy D:\\IFCB_05_20\\D20250521T005712_IFCB135.adc to D:\\test0\\20250521\\7_m\\D20250521T005712_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T005712_IFCB135.roi to D:\\test0\\20250521\\7_m\\D20250521T005712_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T005712_IFCB135.hdr to D:\\test0\\20250521\\7_m\\D20250521T005712_IFCB135.hdr\n",
      "15: D20250521T013141\n",
      "Copy D:\\IFCB_05_20\\D20250521T013141_IFCB135.adc to D:\\test0\\20250521\\8_m\\D20250521T013141_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T013141_IFCB135.roi to D:\\test0\\20250521\\8_m\\D20250521T013141_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T013141_IFCB135.hdr to D:\\test0\\20250521\\8_m\\D20250521T013141_IFCB135.hdr\n",
      "16: D20250521T020730\n",
      "Copy D:\\IFCB_05_20\\D20250521T020730_IFCB135.roi to D:\\test0\\20250521\\10_m\\D20250521T020730_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T020730_IFCB135.hdr to D:\\test0\\20250521\\10_m\\D20250521T020730_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T020730_IFCB135.adc to D:\\test0\\20250521\\10_m\\D20250521T020730_IFCB135.adc\n",
      "17: D20250521T044425\n",
      "Copy D:\\IFCB_05_20\\D20250521T044425_IFCB135.roi to D:\\test0\\20250521\\16_m\\D20250521T044425_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T044425_IFCB135.hdr to D:\\test0\\20250521\\16_m\\D20250521T044425_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T044425_IFCB135.adc to D:\\test0\\20250521\\16_m\\D20250521T044425_IFCB135.adc\n",
      "18: D20250521T052326\n",
      "Copy D:\\IFCB_05_20\\D20250521T052326_IFCB135.roi to D:\\test0\\20250521\\2_m\\D20250521T052326_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T052326_IFCB135.hdr to D:\\test0\\20250521\\2_m\\D20250521T052326_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T052326_IFCB135.adc to D:\\test0\\20250521\\2_m\\D20250521T052326_IFCB135.adc\n",
      "19: missing_data\n",
      "20: D20250521T055710\n",
      "Copy D:\\IFCB_05_20\\D20250521T055710_IFCB135.adc to D:\\test0\\20250521\\7_m\\D20250521T055710_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T055710_IFCB135.roi to D:\\test0\\20250521\\7_m\\D20250521T055710_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T055710_IFCB135.hdr to D:\\test0\\20250521\\7_m\\D20250521T055710_IFCB135.hdr\n",
      "21: D20250521T063140\n",
      "Copy D:\\IFCB_05_20\\D20250521T063140_IFCB135.roi to D:\\test0\\20250521\\8_m\\D20250521T063140_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T063140_IFCB135.hdr to D:\\test0\\20250521\\8_m\\D20250521T063140_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T063140_IFCB135.adc to D:\\test0\\20250521\\8_m\\D20250521T063140_IFCB135.adc\n",
      "22: D20250521T070731\n",
      "Copy D:\\IFCB_05_20\\D20250521T070731_IFCB135.hdr to D:\\test0\\20250521\\10_m\\D20250521T070731_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T070731_IFCB135.roi to D:\\test0\\20250521\\10_m\\D20250521T070731_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T070731_IFCB135.adc to D:\\test0\\20250521\\10_m\\D20250521T070731_IFCB135.adc\n",
      "23: D20250521T075153\n",
      "Copy D:\\IFCB_05_20\\D20250521T075153_IFCB135.roi to D:\\test0\\20250521\\16_m\\D20250521T075153_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T075153_IFCB135.hdr to D:\\test0\\20250521\\16_m\\D20250521T075153_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T075153_IFCB135.adc to D:\\test0\\20250521\\16_m\\D20250521T075153_IFCB135.adc\n",
      "24: D20250521T094427\n",
      "Copy D:\\IFCB_05_20\\D20250521T094427_IFCB135.roi to D:\\test0\\20250521\\2_m\\D20250521T094427_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T094427_IFCB135.hdr to D:\\test0\\20250521\\2_m\\D20250521T094427_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T094427_IFCB135.adc to D:\\test0\\20250521\\2_m\\D20250521T094427_IFCB135.adc\n",
      "25: D20250521T102333\n",
      "Copy D:\\IFCB_05_20\\D20250521T102333_IFCB135.adc to D:\\test0\\20250521\\6.5_m\\D20250521T102333_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T102333_IFCB135.roi to D:\\test0\\20250521\\6.5_m\\D20250521T102333_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T102333_IFCB135.hdr to D:\\test0\\20250521\\6.5_m\\D20250521T102333_IFCB135.hdr\n",
      "26: D20250521T105716\n",
      "Copy D:\\IFCB_05_20\\D20250521T105716_IFCB135.hdr to D:\\test0\\20250521\\7_m\\D20250521T105716_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T105716_IFCB135.roi to D:\\test0\\20250521\\7_m\\D20250521T105716_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T105716_IFCB135.adc to D:\\test0\\20250521\\7_m\\D20250521T105716_IFCB135.adc\n",
      "27: D20250521T113154\n",
      "Copy D:\\IFCB_05_20\\D20250521T113154_IFCB135.adc to D:\\test0\\20250521\\8_m\\D20250521T113154_IFCB135.adc\n",
      "Copy D:\\IFCB_05_20\\D20250521T113154_IFCB135.hdr to D:\\test0\\20250521\\8_m\\D20250521T113154_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T113154_IFCB135.roi to D:\\test0\\20250521\\8_m\\D20250521T113154_IFCB135.roi\n",
      "28: D20250521T120743\n",
      "Copy D:\\IFCB_05_20\\D20250521T120743_IFCB135.roi to D:\\test0\\20250521\\10_m\\D20250521T120743_IFCB135.roi\n",
      "Copy D:\\IFCB_05_20\\D20250521T120743_IFCB135.hdr to D:\\test0\\20250521\\10_m\\D20250521T120743_IFCB135.hdr\n",
      "Copy D:\\IFCB_05_20\\D20250521T120743_IFCB135.adc to D:\\test0\\20250521\\10_m\\D20250521T120743_IFCB135.adc\n",
      "29: missing_data\n"
     ]
    }
   ],
   "source": [
    "timing = list({file.stem.split(\"_\")[0] for file in Path(input_folder_path).iterdir() if file.is_file()})\n",
    "\n",
    "timing_unix = [convert_to_unix_time(time) for time in timing]\n",
    "\n",
    "start_time = min(timing_unix)\n",
    "sorted_timing_unix = sorted([(time-start_time) for time in timing_unix])\n",
    "\n",
    "missing_data = find_missing_data_points(input_folder_path, number_of_clusters, number_of_elements_in_clusters)\n",
    "\n",
    "missing_data_indices = []\n",
    "\n",
    "for cluster_index, missing_indices in missing_data:\n",
    "    absolute_indices_of_missing_data = [cluster_index * number_of_elements_in_clusters + idx for idx in missing_indices]\n",
    "    missing_data_indices.extend(absolute_indices_of_missing_data)\n",
    "\n",
    "print(f\"Missing data indices: {missing_data_indices}\")\n",
    "\n",
    "shifting = 0\n",
    "for idx in missing_data_indices:\n",
    "    sorted_timing_unix.insert(idx + shifting, 'missing_data')\n",
    "    shifting += 1\n",
    "\n",
    "print(f\"Sorted timing with missing data: {sorted_timing_unix}\")\n",
    "print(f\"length: {len(sorted_timing_unix)}\")\n",
    "\n",
    "# Convert back to original format\n",
    "sorted_timing_original = [unix_to_original_format(time+start_time) if time != 'missing_data' else 'missing_data' for time in sorted_timing_unix]\n",
    "# print(f\"Sorted timing in original format: {sorted_timing_original}\")\n",
    "\n",
    "for index, date_time in enumerate(sorted_timing_original):\n",
    "    print(f\"{index}: {date_time}\")\n",
    "\n",
    "    if date_time != 'missing_data':\n",
    "        \n",
    "        pattern = os.path.join(input_folder_path, f\"{date_time}*\")\n",
    "\n",
    "        # Extract YYYYMMDD from the date string\n",
    "        folder_name = date_time[1:9]    \n",
    "    \n",
    "        # Get all matching files\n",
    "        matching_files = glob.glob(pattern)\n",
    "\n",
    "        for file in matching_files:\n",
    "            try:\n",
    "                base_name = os.path.basename(file)\n",
    "\n",
    "                depth = folder_name_map[int(index % number_of_elements_in_clusters)]\n",
    "                desination_folder = os.path.join(output_folder_path, folder_name, depth)\n",
    "                os.makedirs(desination_folder, exist_ok=True)\n",
    "\n",
    "                shutil.copy2(file, os.path.join(desination_folder, base_name))\n",
    "                print(f\"Copy {file} to {os.path.join(desination_folder, base_name)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IGB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
